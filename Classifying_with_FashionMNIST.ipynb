{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HusseinMansourMohd/Data_Minning_final_project/blob/main/Classifying_with_FashionMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVmkx41hJb5e"
      },
      "source": [
        "##1. multilayer perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG5CFh8fe7Cb",
        "outputId": "d8110e11-2fed-465d-fd18-e37e881cd391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 17149113.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 303812.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 4931909.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 23469866.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#load FashionMNIST dataset\n",
        "test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=train_transforms)\n",
        "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=test_transform)\n",
        "train_size = int(0.8 * len(trainset))\n",
        "validation_size = len(trainset) - train_size\n",
        "train_dataset, validation_dataset = random_split(trainset, [train_size, validation_size])\n",
        "trainloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "validationloader = DataLoader(validation_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfomLG4LrAIK",
        "outputId": "31f5238c-b563-4aff-c38f-30e7682f6f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "539ba541-2f4f-4659-c6cb-f25fa6d99396",
        "id": "b4f_rH-srDnu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "[1,  2000] loss: 1.312\n",
            "[1,  4000] loss: 0.770\n",
            "[1,  6000] loss: 0.662\n",
            "[1,  8000] loss: 0.625\n",
            "[1, 10000] loss: 0.587\n",
            "[1, 12000] loss: 0.556\n",
            "[2,  2000] loss: 0.527\n",
            "[2,  4000] loss: 0.523\n",
            "[2,  6000] loss: 0.511\n",
            "[2,  8000] loss: 0.500\n",
            "[2, 10000] loss: 0.499\n",
            "[2, 12000] loss: 0.496\n",
            "[3,  2000] loss: 0.486\n",
            "[3,  4000] loss: 0.471\n",
            "[3,  6000] loss: 0.451\n",
            "[3,  8000] loss: 0.472\n",
            "[3, 10000] loss: 0.461\n",
            "[3, 12000] loss: 0.438\n",
            "[4,  2000] loss: 0.446\n",
            "[4,  4000] loss: 0.431\n",
            "[4,  6000] loss: 0.427\n",
            "[4,  8000] loss: 0.435\n",
            "[4, 10000] loss: 0.446\n",
            "[4, 12000] loss: 0.412\n",
            "[5,  2000] loss: 0.418\n",
            "[5,  4000] loss: 0.434\n",
            "[5,  6000] loss: 0.413\n",
            "[5,  8000] loss: 0.409\n",
            "[5, 10000] loss: 0.391\n",
            "[5, 12000] loss: 0.407\n",
            "[6,  2000] loss: 0.396\n",
            "[6,  4000] loss: 0.395\n",
            "[6,  6000] loss: 0.413\n",
            "[6,  8000] loss: 0.396\n",
            "[6, 10000] loss: 0.383\n",
            "[6, 12000] loss: 0.390\n",
            "[7,  2000] loss: 0.395\n",
            "[7,  4000] loss: 0.378\n",
            "[7,  6000] loss: 0.383\n",
            "[7,  8000] loss: 0.390\n",
            "[7, 10000] loss: 0.374\n",
            "[7, 12000] loss: 0.373\n",
            "[8,  2000] loss: 0.373\n",
            "[8,  4000] loss: 0.373\n",
            "[8,  6000] loss: 0.376\n",
            "[8,  8000] loss: 0.376\n",
            "[8, 10000] loss: 0.370\n",
            "[8, 12000] loss: 0.356\n",
            "[9,  2000] loss: 0.357\n",
            "[9,  4000] loss: 0.369\n",
            "[9,  6000] loss: 0.363\n",
            "[9,  8000] loss: 0.362\n",
            "[9, 10000] loss: 0.357\n",
            "[9, 12000] loss: 0.357\n",
            "[10,  2000] loss: 0.354\n",
            "[10,  4000] loss: 0.347\n",
            "[10,  6000] loss: 0.358\n",
            "[10,  8000] loss: 0.358\n",
            "[10, 10000] loss: 0.352\n",
            "[10, 12000] loss: 0.338\n",
            "[11,  2000] loss: 0.334\n",
            "[11,  4000] loss: 0.356\n",
            "[11,  6000] loss: 0.343\n",
            "[11,  8000] loss: 0.352\n",
            "[11, 10000] loss: 0.338\n",
            "[11, 12000] loss: 0.332\n",
            "[12,  2000] loss: 0.338\n",
            "[12,  4000] loss: 0.347\n",
            "[12,  6000] loss: 0.333\n",
            "[12,  8000] loss: 0.338\n",
            "[12, 10000] loss: 0.332\n",
            "[12, 12000] loss: 0.327\n",
            "[13,  2000] loss: 0.327\n",
            "[13,  4000] loss: 0.333\n",
            "[13,  6000] loss: 0.341\n",
            "[13,  8000] loss: 0.327\n",
            "[13, 10000] loss: 0.326\n",
            "[13, 12000] loss: 0.321\n",
            "[14,  2000] loss: 0.322\n",
            "[14,  4000] loss: 0.323\n",
            "[14,  6000] loss: 0.314\n",
            "[14,  8000] loss: 0.327\n",
            "[14, 10000] loss: 0.328\n",
            "[14, 12000] loss: 0.320\n",
            "[15,  2000] loss: 0.326\n",
            "[15,  4000] loss: 0.310\n",
            "[15,  6000] loss: 0.310\n",
            "[15,  8000] loss: 0.318\n",
            "[15, 10000] loss: 0.309\n",
            "[15, 12000] loss: 0.322\n",
            "[16,  2000] loss: 0.313\n",
            "[16,  4000] loss: 0.309\n",
            "[16,  6000] loss: 0.310\n",
            "[16,  8000] loss: 0.315\n",
            "[16, 10000] loss: 0.315\n",
            "[16, 12000] loss: 0.300\n",
            "[17,  2000] loss: 0.296\n",
            "[17,  4000] loss: 0.307\n",
            "[17,  6000] loss: 0.316\n",
            "[17,  8000] loss: 0.301\n",
            "[17, 10000] loss: 0.301\n",
            "[17, 12000] loss: 0.306\n",
            "[18,  2000] loss: 0.309\n",
            "[18,  4000] loss: 0.301\n",
            "[18,  6000] loss: 0.293\n",
            "[18,  8000] loss: 0.312\n",
            "[18, 10000] loss: 0.291\n",
            "[18, 12000] loss: 0.291\n",
            "[19,  2000] loss: 0.294\n",
            "[19,  4000] loss: 0.299\n",
            "[19,  6000] loss: 0.287\n",
            "[19,  8000] loss: 0.304\n",
            "[19, 10000] loss: 0.298\n",
            "[19, 12000] loss: 0.293\n",
            "[20,  2000] loss: 0.288\n",
            "[20,  4000] loss: 0.291\n",
            "[20,  6000] loss: 0.289\n",
            "[20,  8000] loss: 0.289\n",
            "[20, 10000] loss: 0.284\n",
            "[20, 12000] loss: 0.293\n",
            "[21,  2000] loss: 0.282\n",
            "[21,  4000] loss: 0.290\n",
            "[21,  6000] loss: 0.284\n",
            "[21,  8000] loss: 0.284\n",
            "[21, 10000] loss: 0.288\n",
            "[21, 12000] loss: 0.285\n",
            "[22,  2000] loss: 0.279\n",
            "[22,  4000] loss: 0.277\n",
            "[22,  6000] loss: 0.280\n",
            "[22,  8000] loss: 0.291\n",
            "[22, 10000] loss: 0.279\n",
            "[22, 12000] loss: 0.289\n",
            "[23,  2000] loss: 0.275\n",
            "[23,  4000] loss: 0.270\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.layer_stack(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "#enable GPU\n",
        "model = SimpleNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "\n",
        "#train\n",
        "for epoch in range(50):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765c6dd7-bac4-49c9-dea2-7107fe5d397d",
        "id": "iGQyu-7-sdHV"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            " T-shirt/top      0.113     0.894     0.201      1000\n",
            "     Trouser      0.000     0.000     0.000      1000\n",
            "    Pullover      0.000     0.000     0.000      1000\n",
            "       Dress      0.000     0.000     0.000      1000\n",
            "        Coat      0.000     0.000     0.000      1000\n",
            "      Sandal      0.501     0.650     0.566      1000\n",
            "       Shirt      0.000     0.000     0.000      1000\n",
            "     Sneaker      0.457     0.368     0.408      1000\n",
            "         Bag      0.000     0.000     0.000      1000\n",
            "  Ankle boot      0.000     0.000     0.000      1000\n",
            "\n",
            "    accuracy                          0.191     10000\n",
            "   macro avg      0.107     0.191     0.117     10000\n",
            "weighted avg      0.107     0.191     0.117     10000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "y_pred= []\n",
        "y_true=[]\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=testset.classes, digits=3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egl4Ghs8Iohb"
      },
      "source": [
        "# 2. Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQxiNxro5qPN",
        "outputId": "8509e0f1-261b-4223-ab23-1b8ceffb1123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84      1202\n",
            "           1       0.99      0.97      0.98      1219\n",
            "           2       0.83      0.83      0.83      1205\n",
            "           3       0.86      0.92      0.89      1184\n",
            "           4       0.82      0.85      0.83      1202\n",
            "           5       0.97      0.96      0.96      1211\n",
            "           6       0.75      0.66      0.70      1218\n",
            "           7       0.94      0.96      0.95      1159\n",
            "           8       0.95      0.97      0.96      1197\n",
            "           9       0.97      0.96      0.96      1203\n",
            "\n",
            "    accuracy                           0.89     12000\n",
            "   macro avg       0.89      0.89      0.89     12000\n",
            "weighted avg       0.89      0.89      0.89     12000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83      1000\n",
            "           1       1.00      0.96      0.98      1000\n",
            "           2       0.79      0.81      0.80      1000\n",
            "           3       0.86      0.89      0.88      1000\n",
            "           4       0.80      0.81      0.81      1000\n",
            "           5       0.96      0.96      0.96      1000\n",
            "           6       0.71      0.64      0.67      1000\n",
            "           7       0.94      0.95      0.94      1000\n",
            "           8       0.95      0.97      0.96      1000\n",
            "           9       0.96      0.95      0.96      1000\n",
            "\n",
            "    accuracy                           0.88     10000\n",
            "   macro avg       0.88      0.88      0.88     10000\n",
            "weighted avg       0.88      0.88      0.88     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "X_train = trainset.data.numpy().reshape((trainset.data.shape[0], -1))\n",
        "y_train = trainset.targets.numpy()\n",
        "X_test = testset.data.numpy().reshape((testset.data.shape[0], -1))\n",
        "y_test = testset.targets.numpy()\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "clf = svm.SVC(gamma='scale')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_val = clf.predict(X_val)\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "\n",
        "# Test svm\n",
        "y_pred_test = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWDp9p3GOeBl"
      },
      "source": [
        "#3. Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8i9gMae5q3X",
        "outputId": "b45b91f9-e71d-413d-93c3-b2bd16dc2284"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84      1202\n",
            "           1       1.00      0.97      0.98      1219\n",
            "           2       0.79      0.82      0.80      1205\n",
            "           3       0.87      0.91      0.89      1184\n",
            "           4       0.77      0.83      0.80      1202\n",
            "           5       0.97      0.96      0.97      1211\n",
            "           6       0.75      0.60      0.67      1218\n",
            "           7       0.94      0.94      0.94      1159\n",
            "           8       0.96      0.97      0.96      1197\n",
            "           9       0.95      0.96      0.95      1203\n",
            "\n",
            "    accuracy                           0.88     12000\n",
            "   macro avg       0.88      0.88      0.88     12000\n",
            "weighted avg       0.88      0.88      0.88     12000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83      1000\n",
            "           1       0.99      0.96      0.97      1000\n",
            "           2       0.77      0.80      0.78      1000\n",
            "           3       0.87      0.90      0.89      1000\n",
            "           4       0.76      0.81      0.79      1000\n",
            "           5       0.98      0.96      0.97      1000\n",
            "           6       0.71      0.59      0.64      1000\n",
            "           7       0.93      0.95      0.94      1000\n",
            "           8       0.96      0.97      0.96      1000\n",
            "           9       0.95      0.95      0.95      1000\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred_val = clf.predict(X_val)\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "y_pred_test = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kORYhaXqPGS3"
      },
      "source": [
        "##4. k-Nearest Neighbors (kNN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I34_mQYwAtHl",
        "outputId": "4f1ede22-98f8-4169-ae9a-9d86151a5876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.86      0.80      1202\n",
            "           1       0.99      0.97      0.98      1219\n",
            "           2       0.74      0.80      0.77      1205\n",
            "           3       0.88      0.86      0.87      1184\n",
            "           4       0.78      0.75      0.77      1202\n",
            "           5       0.99      0.85      0.91      1211\n",
            "           6       0.66      0.59      0.62      1218\n",
            "           7       0.89      0.96      0.92      1159\n",
            "           8       0.98      0.93      0.95      1197\n",
            "           9       0.90      0.97      0.93      1203\n",
            "\n",
            "    accuracy                           0.85     12000\n",
            "   macro avg       0.86      0.85      0.85     12000\n",
            "weighted avg       0.85      0.85      0.85     12000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.84      0.79      1000\n",
            "           1       0.98      0.97      0.97      1000\n",
            "           2       0.73      0.79      0.76      1000\n",
            "           3       0.90      0.85      0.87      1000\n",
            "           4       0.77      0.74      0.76      1000\n",
            "           5       0.98      0.85      0.91      1000\n",
            "           6       0.62      0.58      0.60      1000\n",
            "           7       0.89      0.95      0.92      1000\n",
            "           8       0.98      0.93      0.95      1000\n",
            "           9       0.90      0.96      0.93      1000\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred_val = clf.predict(X_val)\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "y_pred_test = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcLGDa1qPKHu"
      },
      "source": [
        "# 5. Gradient Boosting Machines (GBM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjNHwevRMeTl"
      },
      "source": [
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUgLaB34MX5D"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUtRiBZ8AtSR",
        "outputId": "ae7f554b-4d2e-4457-f6ef-f08dcdaf74be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86      1202\n",
            "           1       1.00      0.98      0.99      1219\n",
            "           2       0.83      0.85      0.84      1205\n",
            "           3       0.89      0.92      0.91      1184\n",
            "           4       0.83      0.84      0.84      1202\n",
            "           5       0.98      0.97      0.98      1211\n",
            "           6       0.77      0.69      0.73      1218\n",
            "           7       0.96      0.96      0.96      1159\n",
            "           8       0.98      0.97      0.97      1197\n",
            "           9       0.97      0.97      0.97      1203\n",
            "\n",
            "    accuracy                           0.90     12000\n",
            "   macro avg       0.90      0.90      0.90     12000\n",
            "weighted avg       0.90      0.90      0.90     12000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.85      1000\n",
            "           1       0.99      0.97      0.98      1000\n",
            "           2       0.79      0.82      0.81      1000\n",
            "           3       0.90      0.90      0.90      1000\n",
            "           4       0.81      0.83      0.82      1000\n",
            "           5       0.99      0.97      0.98      1000\n",
            "           6       0.72      0.66      0.69      1000\n",
            "           7       0.95      0.97      0.96      1000\n",
            "           8       0.97      0.97      0.97      1000\n",
            "           9       0.96      0.97      0.96      1000\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "clf = XGBClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred_val = clf.predict(X_val)\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "y_pred_test = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YerOvDp0MbXo"
      },
      "source": [
        "#6. Exploring Neural Network more (Convolutional Neural Networks)\n",
        "\n",
        "1.   Add Multiple Layers\n",
        "2.   or we can try ResNet(if it's needed , probably NOT!!)\n",
        "3.   check multiple opimizer also check gradient learning rates.\n",
        "4.   add more data augmention.\n",
        "5.   maxpool avgpool\n",
        "\n",
        "add more visualizition for the report\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4bbe276-9ae1-4866-bcdc-c0b8e2d6a5e2",
        "id": "aTS4rzuYvXcJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.304\n",
            "[1,  4000] loss: 2.303\n",
            "[1,  6000] loss: 2.302\n",
            "[1,  8000] loss: 2.301\n",
            "[1, 10000] loss: 2.299\n",
            "[1, 12000] loss: 2.298\n",
            "[2,  2000] loss: 2.295\n",
            "[2,  4000] loss: 2.292\n",
            "[2,  6000] loss: 2.283\n",
            "[2,  8000] loss: 2.266\n",
            "[2, 10000] loss: 2.208\n",
            "[2, 12000] loss: 2.085\n",
            "[3,  2000] loss: 1.982\n",
            "[3,  4000] loss: 1.899\n",
            "[3,  6000] loss: 1.825\n",
            "[3,  8000] loss: 1.735\n",
            "[3, 10000] loss: 1.657\n",
            "[3, 12000] loss: 1.616\n",
            "[4,  2000] loss: 1.560\n",
            "[4,  4000] loss: 1.511\n",
            "[4,  6000] loss: 1.466\n",
            "[4,  8000] loss: 1.390\n",
            "[4, 10000] loss: 1.336\n",
            "[4, 12000] loss: 1.287\n",
            "[5,  2000] loss: 1.229\n",
            "[5,  4000] loss: 1.203\n",
            "[5,  6000] loss: 1.148\n",
            "[5,  8000] loss: 1.114\n",
            "[5, 10000] loss: 1.067\n",
            "[5, 12000] loss: 1.014\n",
            "[6,  2000] loss: 0.999\n",
            "[6,  4000] loss: 0.957\n",
            "[6,  6000] loss: 0.946\n",
            "[6,  8000] loss: 0.927\n",
            "[6, 10000] loss: 0.916\n",
            "[6, 12000] loss: 0.910\n",
            "[7,  2000] loss: 0.882\n",
            "[7,  4000] loss: 0.875\n",
            "[7,  6000] loss: 0.860\n",
            "[7,  8000] loss: 0.841\n",
            "[7, 10000] loss: 0.833\n",
            "[7, 12000] loss: 0.833\n",
            "[8,  2000] loss: 0.819\n",
            "[8,  4000] loss: 0.807\n",
            "[8,  6000] loss: 0.792\n",
            "[8,  8000] loss: 0.809\n",
            "[8, 10000] loss: 0.783\n",
            "[8, 12000] loss: 0.774\n",
            "[9,  2000] loss: 0.764\n",
            "[9,  4000] loss: 0.754\n",
            "[9,  6000] loss: 0.736\n",
            "[9,  8000] loss: 0.750\n",
            "[9, 10000] loss: 0.767\n",
            "[9, 12000] loss: 0.743\n",
            "[10,  2000] loss: 0.729\n",
            "[10,  4000] loss: 0.725\n",
            "[10,  6000] loss: 0.721\n",
            "[10,  8000] loss: 0.682\n",
            "[10, 10000] loss: 0.710\n",
            "[10, 12000] loss: 0.697\n",
            "[11,  2000] loss: 0.686\n",
            "[11,  4000] loss: 0.681\n",
            "[11,  6000] loss: 0.689\n",
            "[11,  8000] loss: 0.675\n",
            "[11, 10000] loss: 0.668\n",
            "[11, 12000] loss: 0.656\n",
            "[12,  2000] loss: 0.681\n",
            "[12,  4000] loss: 0.658\n",
            "[12,  6000] loss: 0.654\n",
            "[12,  8000] loss: 0.650\n",
            "[12, 10000] loss: 0.634\n",
            "[12, 12000] loss: 0.631\n",
            "[13,  2000] loss: 0.644\n",
            "[13,  4000] loss: 0.631\n",
            "[13,  6000] loss: 0.628\n",
            "[13,  8000] loss: 0.629\n",
            "[13, 10000] loss: 0.617\n",
            "[13, 12000] loss: 0.615\n",
            "[14,  2000] loss: 0.609\n",
            "[14,  4000] loss: 0.612\n",
            "[14,  6000] loss: 0.613\n",
            "[14,  8000] loss: 0.597\n",
            "[14, 10000] loss: 0.599\n",
            "[14, 12000] loss: 0.617\n",
            "[15,  2000] loss: 0.607\n",
            "[15,  4000] loss: 0.591\n",
            "[15,  6000] loss: 0.610\n",
            "[15,  8000] loss: 0.574\n",
            "[15, 10000] loss: 0.575\n",
            "[15, 12000] loss: 0.576\n",
            "[16,  2000] loss: 0.582\n",
            "[16,  4000] loss: 0.574\n",
            "[16,  6000] loss: 0.573\n",
            "[16,  8000] loss: 0.584\n",
            "[16, 10000] loss: 0.565\n",
            "[16, 12000] loss: 0.559\n",
            "[17,  2000] loss: 0.551\n",
            "[17,  4000] loss: 0.569\n",
            "[17,  6000] loss: 0.574\n",
            "[17,  8000] loss: 0.548\n",
            "[17, 10000] loss: 0.555\n",
            "[17, 12000] loss: 0.554\n",
            "[18,  2000] loss: 0.552\n",
            "[18,  4000] loss: 0.545\n",
            "[18,  6000] loss: 0.557\n",
            "[18,  8000] loss: 0.551\n",
            "[18, 10000] loss: 0.550\n",
            "[18, 12000] loss: 0.517\n",
            "[19,  2000] loss: 0.545\n",
            "[19,  4000] loss: 0.540\n",
            "[19,  6000] loss: 0.527\n",
            "[19,  8000] loss: 0.525\n",
            "[19, 10000] loss: 0.525\n",
            "[19, 12000] loss: 0.526\n",
            "[20,  2000] loss: 0.521\n",
            "[20,  4000] loss: 0.513\n",
            "[20,  6000] loss: 0.535\n",
            "[20,  8000] loss: 0.529\n",
            "[20, 10000] loss: 0.511\n",
            "[20, 12000] loss: 0.510\n",
            "[21,  2000] loss: 0.526\n",
            "[21,  4000] loss: 0.512\n",
            "[21,  6000] loss: 0.497\n",
            "[21,  8000] loss: 0.509\n",
            "[21, 10000] loss: 0.502\n",
            "[21, 12000] loss: 0.504\n",
            "[22,  2000] loss: 0.513\n",
            "[22,  4000] loss: 0.487\n",
            "[22,  6000] loss: 0.506\n",
            "[22,  8000] loss: 0.493\n",
            "[22, 10000] loss: 0.501\n",
            "[22, 12000] loss: 0.495\n",
            "[23,  2000] loss: 0.485\n",
            "[23,  4000] loss: 0.492\n",
            "[23,  6000] loss: 0.491\n",
            "[23,  8000] loss: 0.500\n",
            "[23, 10000] loss: 0.477\n",
            "[23, 12000] loss: 0.487\n",
            "[24,  2000] loss: 0.471\n",
            "[24,  4000] loss: 0.484\n",
            "[24,  6000] loss: 0.476\n",
            "[24,  8000] loss: 0.503\n",
            "[24, 10000] loss: 0.483\n",
            "[24, 12000] loss: 0.465\n",
            "[25,  2000] loss: 0.471\n",
            "[25,  4000] loss: 0.474\n",
            "[25,  6000] loss: 0.484\n",
            "[25,  8000] loss: 0.477\n",
            "[25, 10000] loss: 0.471\n",
            "[25, 12000] loss: 0.458\n",
            "[26,  2000] loss: 0.471\n",
            "[26,  4000] loss: 0.457\n",
            "[26,  6000] loss: 0.470\n",
            "[26,  8000] loss: 0.449\n",
            "[26, 10000] loss: 0.464\n",
            "[26, 12000] loss: 0.464\n",
            "[27,  2000] loss: 0.447\n",
            "[27,  4000] loss: 0.446\n",
            "[27,  6000] loss: 0.457\n",
            "[27,  8000] loss: 0.461\n",
            "[27, 10000] loss: 0.449\n",
            "[27, 12000] loss: 0.450\n",
            "[28,  2000] loss: 0.441\n",
            "[28,  4000] loss: 0.449\n",
            "[28,  6000] loss: 0.448\n",
            "[28,  8000] loss: 0.439\n",
            "[28, 10000] loss: 0.449\n",
            "[28, 12000] loss: 0.447\n",
            "[29,  2000] loss: 0.441\n",
            "[29,  4000] loss: 0.430\n",
            "[29,  6000] loss: 0.433\n",
            "[29,  8000] loss: 0.432\n",
            "[29, 10000] loss: 0.443\n",
            "[29, 12000] loss: 0.432\n",
            "[30,  2000] loss: 0.421\n",
            "[30,  4000] loss: 0.445\n",
            "[30,  6000] loss: 0.430\n",
            "[30,  8000] loss: 0.433\n",
            "[30, 10000] loss: 0.415\n",
            "[30, 12000] loss: 0.436\n",
            "[31,  2000] loss: 0.423\n",
            "[31,  4000] loss: 0.434\n",
            "[31,  6000] loss: 0.427\n",
            "[31,  8000] loss: 0.414\n",
            "[31, 10000] loss: 0.411\n",
            "[31, 12000] loss: 0.412\n",
            "[32,  2000] loss: 0.413\n",
            "[32,  4000] loss: 0.411\n",
            "[32,  6000] loss: 0.420\n",
            "[32,  8000] loss: 0.398\n",
            "[32, 10000] loss: 0.423\n",
            "[32, 12000] loss: 0.416\n",
            "[33,  2000] loss: 0.411\n",
            "[33,  4000] loss: 0.397\n",
            "[33,  6000] loss: 0.416\n",
            "[33,  8000] loss: 0.402\n",
            "[33, 10000] loss: 0.406\n",
            "[33, 12000] loss: 0.404\n",
            "[34,  2000] loss: 0.388\n",
            "[34,  4000] loss: 0.405\n",
            "[34,  6000] loss: 0.412\n",
            "[34,  8000] loss: 0.393\n",
            "[34, 10000] loss: 0.392\n",
            "[34, 12000] loss: 0.406\n",
            "[35,  2000] loss: 0.391\n",
            "[35,  4000] loss: 0.398\n",
            "[35,  6000] loss: 0.385\n",
            "[35,  8000] loss: 0.399\n",
            "[35, 10000] loss: 0.398\n",
            "[35, 12000] loss: 0.393\n",
            "[36,  2000] loss: 0.375\n",
            "[36,  4000] loss: 0.393\n",
            "[36,  6000] loss: 0.397\n",
            "[36,  8000] loss: 0.398\n",
            "[36, 10000] loss: 0.381\n",
            "[36, 12000] loss: 0.377\n",
            "[37,  2000] loss: 0.381\n",
            "[37,  4000] loss: 0.388\n",
            "[37,  6000] loss: 0.384\n",
            "[37,  8000] loss: 0.370\n",
            "[37, 10000] loss: 0.373\n",
            "[37, 12000] loss: 0.398\n",
            "[38,  2000] loss: 0.377\n",
            "[38,  4000] loss: 0.370\n",
            "[38,  6000] loss: 0.369\n",
            "[38,  8000] loss: 0.373\n",
            "[38, 10000] loss: 0.387\n",
            "[38, 12000] loss: 0.371\n",
            "[39,  2000] loss: 0.372\n",
            "[39,  4000] loss: 0.359\n",
            "[39,  6000] loss: 0.373\n",
            "[39,  8000] loss: 0.373\n",
            "[39, 10000] loss: 0.367\n",
            "[39, 12000] loss: 0.376\n",
            "[40,  2000] loss: 0.357\n",
            "[40,  4000] loss: 0.372\n",
            "[40,  6000] loss: 0.370\n",
            "[40,  8000] loss: 0.360\n",
            "[40, 10000] loss: 0.366\n",
            "[40, 12000] loss: 0.350\n",
            "[41,  2000] loss: 0.368\n",
            "[41,  4000] loss: 0.361\n",
            "[41,  6000] loss: 0.352\n",
            "[41,  8000] loss: 0.350\n",
            "[41, 10000] loss: 0.364\n",
            "[41, 12000] loss: 0.349\n",
            "[42,  2000] loss: 0.352\n",
            "[42,  4000] loss: 0.366\n",
            "[42,  6000] loss: 0.347\n",
            "[42,  8000] loss: 0.344\n",
            "[42, 10000] loss: 0.351\n",
            "[42, 12000] loss: 0.359\n",
            "[43,  2000] loss: 0.351\n",
            "[43,  4000] loss: 0.344\n",
            "[43,  6000] loss: 0.360\n",
            "[43,  8000] loss: 0.351\n",
            "[43, 10000] loss: 0.354\n",
            "[43, 12000] loss: 0.343\n",
            "[44,  2000] loss: 0.355\n",
            "[44,  4000] loss: 0.336\n",
            "[44,  6000] loss: 0.340\n",
            "[44,  8000] loss: 0.334\n",
            "[44, 10000] loss: 0.353\n",
            "[44, 12000] loss: 0.345\n",
            "[45,  2000] loss: 0.332\n",
            "[45,  4000] loss: 0.343\n",
            "[45,  6000] loss: 0.340\n",
            "[45,  8000] loss: 0.326\n",
            "[45, 10000] loss: 0.350\n",
            "[45, 12000] loss: 0.337\n",
            "[46,  2000] loss: 0.326\n",
            "[46,  4000] loss: 0.326\n",
            "[46,  6000] loss: 0.328\n",
            "[46,  8000] loss: 0.338\n",
            "[46, 10000] loss: 0.336\n",
            "[46, 12000] loss: 0.352\n",
            "[47,  2000] loss: 0.337\n",
            "[47,  4000] loss: 0.334\n",
            "[47,  6000] loss: 0.325\n",
            "[47,  8000] loss: 0.329\n",
            "[47, 10000] loss: 0.329\n",
            "[47, 12000] loss: 0.316\n",
            "[48,  2000] loss: 0.340\n",
            "[48,  4000] loss: 0.325\n",
            "[48,  6000] loss: 0.333\n",
            "[48,  8000] loss: 0.330\n",
            "[48, 10000] loss: 0.313\n",
            "[48, 12000] loss: 0.316\n",
            "[49,  2000] loss: 0.327\n",
            "[49,  4000] loss: 0.323\n",
            "[49,  6000] loss: 0.323\n",
            "[49,  8000] loss: 0.341\n",
            "[49, 10000] loss: 0.313\n",
            "[49, 12000] loss: 0.312\n",
            "[50,  2000] loss: 0.307\n",
            "[50,  4000] loss: 0.323\n",
            "[50,  6000] loss: 0.318\n",
            "[50,  8000] loss: 0.327\n",
            "[50, 10000] loss: 0.327\n",
            "[50, 12000] loss: 0.311\n",
            "[51,  2000] loss: 0.321\n",
            "[51,  4000] loss: 0.312\n",
            "[51,  6000] loss: 0.321\n",
            "[51,  8000] loss: 0.306\n",
            "[51, 10000] loss: 0.319\n",
            "[51, 12000] loss: 0.315\n",
            "[52,  2000] loss: 0.313\n",
            "[52,  4000] loss: 0.307\n",
            "[52,  6000] loss: 0.307\n",
            "[52,  8000] loss: 0.328\n",
            "[52, 10000] loss: 0.298\n",
            "[52, 12000] loss: 0.326\n",
            "[53,  2000] loss: 0.309\n",
            "[53,  4000] loss: 0.317\n",
            "[53,  6000] loss: 0.300\n",
            "[53,  8000] loss: 0.310\n",
            "[53, 10000] loss: 0.318\n",
            "[53, 12000] loss: 0.305\n",
            "[54,  2000] loss: 0.304\n",
            "[54,  4000] loss: 0.297\n",
            "[54,  6000] loss: 0.307\n",
            "[54,  8000] loss: 0.295\n",
            "[54, 10000] loss: 0.324\n",
            "[54, 12000] loss: 0.311\n",
            "[55,  2000] loss: 0.303\n",
            "[55,  4000] loss: 0.308\n",
            "[55,  6000] loss: 0.303\n",
            "[55,  8000] loss: 0.307\n",
            "[55, 10000] loss: 0.301\n",
            "[55, 12000] loss: 0.280\n",
            "[56,  2000] loss: 0.306\n",
            "[56,  4000] loss: 0.301\n",
            "[56,  6000] loss: 0.296\n",
            "[56,  8000] loss: 0.292\n",
            "[56, 10000] loss: 0.299\n",
            "[56, 12000] loss: 0.310\n",
            "[57,  2000] loss: 0.287\n",
            "[57,  4000] loss: 0.301\n",
            "[57,  6000] loss: 0.299\n",
            "[57,  8000] loss: 0.303\n",
            "[57, 10000] loss: 0.297\n",
            "[57, 12000] loss: 0.297\n",
            "[58,  2000] loss: 0.302\n",
            "[58,  4000] loss: 0.298\n",
            "[58,  6000] loss: 0.293\n",
            "[58,  8000] loss: 0.303\n",
            "[58, 10000] loss: 0.288\n",
            "[58, 12000] loss: 0.280\n",
            "[59,  2000] loss: 0.281\n",
            "[59,  4000] loss: 0.288\n",
            "[59,  6000] loss: 0.303\n",
            "[59,  8000] loss: 0.288\n",
            "[59, 10000] loss: 0.297\n",
            "[59, 12000] loss: 0.299\n",
            "[60,  2000] loss: 0.278\n",
            "[60,  4000] loss: 0.282\n",
            "[60,  6000] loss: 0.299\n",
            "[60,  8000] loss: 0.296\n",
            "[60, 10000] loss: 0.278\n",
            "[60, 12000] loss: 0.299\n",
            "[61,  2000] loss: 0.280\n",
            "[61,  4000] loss: 0.283\n",
            "[61,  6000] loss: 0.289\n",
            "[61,  8000] loss: 0.299\n",
            "[61, 10000] loss: 0.282\n",
            "[61, 12000] loss: 0.278\n",
            "[62,  2000] loss: 0.287\n",
            "[62,  4000] loss: 0.301\n",
            "[62,  6000] loss: 0.275\n",
            "[62,  8000] loss: 0.273\n",
            "[62, 10000] loss: 0.286\n",
            "[62, 12000] loss: 0.292\n",
            "[63,  2000] loss: 0.277\n",
            "[63,  4000] loss: 0.284\n",
            "[63,  6000] loss: 0.281\n",
            "[63,  8000] loss: 0.280\n",
            "[63, 10000] loss: 0.289\n",
            "[63, 12000] loss: 0.277\n",
            "[64,  2000] loss: 0.284\n",
            "[64,  4000] loss: 0.286\n",
            "[64,  6000] loss: 0.267\n",
            "[64,  8000] loss: 0.272\n",
            "[64, 10000] loss: 0.276\n",
            "[64, 12000] loss: 0.281\n",
            "[65,  2000] loss: 0.281\n",
            "[65,  4000] loss: 0.283\n",
            "[65,  6000] loss: 0.277\n",
            "[65,  8000] loss: 0.278\n",
            "[65, 10000] loss: 0.267\n",
            "[65, 12000] loss: 0.267\n",
            "[66,  2000] loss: 0.270\n",
            "[66,  4000] loss: 0.267\n",
            "[66,  6000] loss: 0.279\n",
            "[66,  8000] loss: 0.285\n",
            "[66, 10000] loss: 0.268\n",
            "[66, 12000] loss: 0.278\n",
            "[67,  2000] loss: 0.272\n",
            "[67,  4000] loss: 0.270\n",
            "[67,  6000] loss: 0.276\n",
            "[67,  8000] loss: 0.283\n",
            "[67, 10000] loss: 0.270\n",
            "[67, 12000] loss: 0.268\n",
            "[68,  2000] loss: 0.279\n",
            "[68,  4000] loss: 0.277\n",
            "[68,  6000] loss: 0.270\n",
            "[68,  8000] loss: 0.272\n",
            "[68, 10000] loss: 0.263\n",
            "[68, 12000] loss: 0.260\n",
            "[69,  2000] loss: 0.263\n",
            "[69,  4000] loss: 0.262\n",
            "[69,  6000] loss: 0.279\n",
            "[69,  8000] loss: 0.263\n",
            "[69, 10000] loss: 0.266\n",
            "[69, 12000] loss: 0.269\n",
            "[70,  2000] loss: 0.271\n",
            "[70,  4000] loss: 0.262\n",
            "[70,  6000] loss: 0.253\n",
            "[70,  8000] loss: 0.262\n",
            "[70, 10000] loss: 0.267\n",
            "[70, 12000] loss: 0.271\n",
            "[71,  2000] loss: 0.259\n",
            "[71,  4000] loss: 0.275\n",
            "[71,  6000] loss: 0.267\n",
            "[71,  8000] loss: 0.262\n",
            "[71, 10000] loss: 0.258\n",
            "[71, 12000] loss: 0.259\n",
            "[72,  2000] loss: 0.248\n",
            "[72,  4000] loss: 0.259\n",
            "[72,  6000] loss: 0.268\n",
            "[72,  8000] loss: 0.263\n",
            "[72, 10000] loss: 0.265\n",
            "[72, 12000] loss: 0.257\n",
            "[73,  2000] loss: 0.258\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define complex network for FashionMNIST\n",
        "class ComplexFashionMNISTNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ComplexFashionMNISTNet, self).__init__()\n",
        "        # Define layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc1 = nn.Linear(128, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional layers with ReLU and pooling\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.avgpool(x)  # Global Average Pooling\n",
        "        x = torch.flatten(x, 1)\n",
        "        # Fully connected layers with dropout\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "    transforms.RandomErasing()\n",
        "])\n",
        "\n",
        "\n",
        "model = ComplexFashionMNISTNet().to(device)\n",
        "\n",
        "\n",
        "\n",
        "#enable GPU\n",
        "model = ComplexFashionMNISTNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "\n",
        "#train\n",
        "LOSSES = []\n",
        "EPOCHES = []\n",
        "for epoch in range(100):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "    LOSSES.append(running_loss)\n",
        "    EPOCHES.append(epoch)\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA6rvDA17vAx"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "y_pred= []\n",
        "y_true=[]\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=testset.classes, digits=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBqYHAxeHhmm"
      },
      "outputs": [],
      "source": [
        "# visualize activation in\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "model.conv3.register_forward_hook(get_activation('conv2'))\n",
        "model.fc3.register_forward_hook(get_activation('fc3'))\n",
        "\n",
        "images, _ = next(iter(testloader))\n",
        "output = model(images)\n",
        "\n",
        "def visualize_feature_maps(layer_activations):\n",
        "    num_feature_maps = layer_activations.size(1)\n",
        "    fig, axarr = plt.subplots(min(num_feature_maps, 4))\n",
        "    for idx in range(min(num_feature_maps, 4)):\n",
        "        feature_map = layer_activations[0][idx]\n",
        "        axarr[idx].imshow(feature_map.cpu().numpy(), cmap=\"gray\")\n",
        "        axarr[idx].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "visualize_feature_maps(activation['conv2'])\n",
        "\n",
        "def visualize_fc_layer(layer_weights):\n",
        "    plt.imshow(layer_weights.cpu().numpy(), cmap=\"coolwarm\")\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "visualize_fc_layer(model.fc3.weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CID7ltmHhu3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip0mX4diHigc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}