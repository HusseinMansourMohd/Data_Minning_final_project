{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HusseinMansourMohd/Data_Minning_final_project/blob/main/Classifying_with_FashionMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVmkx41hJb5e"
      },
      "source": [
        "##1. multilayer perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG5CFh8fe7Cb",
        "outputId": "f465afb7-13d6-4474-87c6-801bb7deffe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 14357366.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 272817.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5033136.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 19683023.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#load FashionMNIST dataset\n",
        "test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=train_transforms)\n",
        "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=test_transform)\n",
        "train_size = int(0.8 * len(trainset))\n",
        "validation_size = len(trainset) - train_size\n",
        "train_dataset, validation_dataset = random_split(trainset, [train_size, validation_size])\n",
        "trainloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "validationloader = DataLoader(validation_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfomLG4LrAIK",
        "outputId": "efcbfd17-325a-4f66-e5b1-0f45a194a6c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4f_rH-srDnu",
        "outputId": "539ba541-2f4f-4659-c6cb-f25fa6d99396"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n",
            "[1,  2000] loss: 1.312\n",
            "[1,  4000] loss: 0.770\n",
            "[1,  6000] loss: 0.662\n",
            "[1,  8000] loss: 0.625\n",
            "[1, 10000] loss: 0.587\n",
            "[1, 12000] loss: 0.556\n",
            "[2,  2000] loss: 0.527\n",
            "[2,  4000] loss: 0.523\n",
            "[2,  6000] loss: 0.511\n",
            "[2,  8000] loss: 0.500\n",
            "[2, 10000] loss: 0.499\n",
            "[2, 12000] loss: 0.496\n",
            "[3,  2000] loss: 0.486\n",
            "[3,  4000] loss: 0.471\n",
            "[3,  6000] loss: 0.451\n",
            "[3,  8000] loss: 0.472\n",
            "[3, 10000] loss: 0.461\n",
            "[3, 12000] loss: 0.438\n",
            "[4,  2000] loss: 0.446\n",
            "[4,  4000] loss: 0.431\n",
            "[4,  6000] loss: 0.427\n",
            "[4,  8000] loss: 0.435\n",
            "[4, 10000] loss: 0.446\n",
            "[4, 12000] loss: 0.412\n",
            "[5,  2000] loss: 0.418\n",
            "[5,  4000] loss: 0.434\n",
            "[5,  6000] loss: 0.413\n",
            "[5,  8000] loss: 0.409\n",
            "[5, 10000] loss: 0.391\n",
            "[5, 12000] loss: 0.407\n",
            "[6,  2000] loss: 0.396\n",
            "[6,  4000] loss: 0.395\n",
            "[6,  6000] loss: 0.413\n",
            "[6,  8000] loss: 0.396\n",
            "[6, 10000] loss: 0.383\n",
            "[6, 12000] loss: 0.390\n",
            "[7,  2000] loss: 0.395\n",
            "[7,  4000] loss: 0.378\n",
            "[7,  6000] loss: 0.383\n",
            "[7,  8000] loss: 0.390\n",
            "[7, 10000] loss: 0.374\n",
            "[7, 12000] loss: 0.373\n",
            "[8,  2000] loss: 0.373\n",
            "[8,  4000] loss: 0.373\n",
            "[8,  6000] loss: 0.376\n",
            "[8,  8000] loss: 0.376\n",
            "[8, 10000] loss: 0.370\n",
            "[8, 12000] loss: 0.356\n",
            "[9,  2000] loss: 0.357\n",
            "[9,  4000] loss: 0.369\n",
            "[9,  6000] loss: 0.363\n",
            "[9,  8000] loss: 0.362\n",
            "[9, 10000] loss: 0.357\n",
            "[9, 12000] loss: 0.357\n",
            "[10,  2000] loss: 0.354\n",
            "[10,  4000] loss: 0.347\n",
            "[10,  6000] loss: 0.358\n",
            "[10,  8000] loss: 0.358\n",
            "[10, 10000] loss: 0.352\n",
            "[10, 12000] loss: 0.338\n",
            "[11,  2000] loss: 0.334\n",
            "[11,  4000] loss: 0.356\n",
            "[11,  6000] loss: 0.343\n",
            "[11,  8000] loss: 0.352\n",
            "[11, 10000] loss: 0.338\n",
            "[11, 12000] loss: 0.332\n",
            "[12,  2000] loss: 0.338\n",
            "[12,  4000] loss: 0.347\n",
            "[12,  6000] loss: 0.333\n",
            "[12,  8000] loss: 0.338\n",
            "[12, 10000] loss: 0.332\n",
            "[12, 12000] loss: 0.327\n",
            "[13,  2000] loss: 0.327\n",
            "[13,  4000] loss: 0.333\n",
            "[13,  6000] loss: 0.341\n",
            "[13,  8000] loss: 0.327\n",
            "[13, 10000] loss: 0.326\n",
            "[13, 12000] loss: 0.321\n",
            "[14,  2000] loss: 0.322\n",
            "[14,  4000] loss: 0.323\n",
            "[14,  6000] loss: 0.314\n",
            "[14,  8000] loss: 0.327\n",
            "[14, 10000] loss: 0.328\n",
            "[14, 12000] loss: 0.320\n",
            "[15,  2000] loss: 0.326\n",
            "[15,  4000] loss: 0.310\n",
            "[15,  6000] loss: 0.310\n",
            "[15,  8000] loss: 0.318\n",
            "[15, 10000] loss: 0.309\n",
            "[15, 12000] loss: 0.322\n",
            "[16,  2000] loss: 0.313\n",
            "[16,  4000] loss: 0.309\n",
            "[16,  6000] loss: 0.310\n",
            "[16,  8000] loss: 0.315\n",
            "[16, 10000] loss: 0.315\n",
            "[16, 12000] loss: 0.300\n",
            "[17,  2000] loss: 0.296\n",
            "[17,  4000] loss: 0.307\n",
            "[17,  6000] loss: 0.316\n",
            "[17,  8000] loss: 0.301\n",
            "[17, 10000] loss: 0.301\n",
            "[17, 12000] loss: 0.306\n",
            "[18,  2000] loss: 0.309\n",
            "[18,  4000] loss: 0.301\n",
            "[18,  6000] loss: 0.293\n",
            "[18,  8000] loss: 0.312\n",
            "[18, 10000] loss: 0.291\n",
            "[18, 12000] loss: 0.291\n",
            "[19,  2000] loss: 0.294\n",
            "[19,  4000] loss: 0.299\n",
            "[19,  6000] loss: 0.287\n",
            "[19,  8000] loss: 0.304\n",
            "[19, 10000] loss: 0.298\n",
            "[19, 12000] loss: 0.293\n",
            "[20,  2000] loss: 0.288\n",
            "[20,  4000] loss: 0.291\n",
            "[20,  6000] loss: 0.289\n",
            "[20,  8000] loss: 0.289\n",
            "[20, 10000] loss: 0.284\n",
            "[20, 12000] loss: 0.293\n",
            "[21,  2000] loss: 0.282\n",
            "[21,  4000] loss: 0.290\n",
            "[21,  6000] loss: 0.284\n",
            "[21,  8000] loss: 0.284\n",
            "[21, 10000] loss: 0.288\n",
            "[21, 12000] loss: 0.285\n",
            "[22,  2000] loss: 0.279\n",
            "[22,  4000] loss: 0.277\n",
            "[22,  6000] loss: 0.280\n",
            "[22,  8000] loss: 0.291\n",
            "[22, 10000] loss: 0.279\n",
            "[22, 12000] loss: 0.289\n",
            "[23,  2000] loss: 0.275\n",
            "[23,  4000] loss: 0.270\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.layer_stack(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "#enable GPU\n",
        "model = SimpleNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "\n",
        "#train\n",
        "for epoch in range(50):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGQyu-7-sdHV",
        "outputId": "765c6dd7-bac4-49c9-dea2-7107fe5d397d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            " T-shirt/top      0.113     0.894     0.201      1000\n",
            "     Trouser      0.000     0.000     0.000      1000\n",
            "    Pullover      0.000     0.000     0.000      1000\n",
            "       Dress      0.000     0.000     0.000      1000\n",
            "        Coat      0.000     0.000     0.000      1000\n",
            "      Sandal      0.501     0.650     0.566      1000\n",
            "       Shirt      0.000     0.000     0.000      1000\n",
            "     Sneaker      0.457     0.368     0.408      1000\n",
            "         Bag      0.000     0.000     0.000      1000\n",
            "  Ankle boot      0.000     0.000     0.000      1000\n",
            "\n",
            "    accuracy                          0.191     10000\n",
            "   macro avg      0.107     0.191     0.117     10000\n",
            "weighted avg      0.107     0.191     0.117     10000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "y_pred= []\n",
        "y_true=[]\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=testset.classes, digits=3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egl4Ghs8Iohb"
      },
      "source": [
        "# 2. Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQxiNxro5qPN",
        "outputId": "8509e0f1-261b-4223-ab23-1b8ceffb1123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84      1202\n",
            "           1       0.99      0.97      0.98      1219\n",
            "           2       0.83      0.83      0.83      1205\n",
            "           3       0.86      0.92      0.89      1184\n",
            "           4       0.82      0.85      0.83      1202\n",
            "           5       0.97      0.96      0.96      1211\n",
            "           6       0.75      0.66      0.70      1218\n",
            "           7       0.94      0.96      0.95      1159\n",
            "           8       0.95      0.97      0.96      1197\n",
            "           9       0.97      0.96      0.96      1203\n",
            "\n",
            "    accuracy                           0.89     12000\n",
            "   macro avg       0.89      0.89      0.89     12000\n",
            "weighted avg       0.89      0.89      0.89     12000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83      1000\n",
            "           1       1.00      0.96      0.98      1000\n",
            "           2       0.79      0.81      0.80      1000\n",
            "           3       0.86      0.89      0.88      1000\n",
            "           4       0.80      0.81      0.81      1000\n",
            "           5       0.96      0.96      0.96      1000\n",
            "           6       0.71      0.64      0.67      1000\n",
            "           7       0.94      0.95      0.94      1000\n",
            "           8       0.95      0.97      0.96      1000\n",
            "           9       0.96      0.95      0.96      1000\n",
            "\n",
            "    accuracy                           0.88     10000\n",
            "   macro avg       0.88      0.88      0.88     10000\n",
            "weighted avg       0.88      0.88      0.88     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "X_train = trainset.data.numpy().reshape((trainset.data.shape[0], -1))\n",
        "y_train = trainset.targets.numpy()\n",
        "X_test = testset.data.numpy().reshape((testset.data.shape[0], -1))\n",
        "y_test = testset.targets.numpy()\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "clf = svm.SVC(gamma='scale')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_val = clf.predict(X_val)\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "\n",
        "# Test svm\n",
        "y_pred_test = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWDp9p3GOeBl"
      },
      "source": [
        "#3. Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8i9gMae5q3X",
        "outputId": "b45b91f9-e71d-413d-93c3-b2bd16dc2284"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84      1202\n",
            "           1       1.00      0.97      0.98      1219\n",
            "           2       0.79      0.82      0.80      1205\n",
            "           3       0.87      0.91      0.89      1184\n",
            "           4       0.77      0.83      0.80      1202\n",
            "           5       0.97      0.96      0.97      1211\n",
            "           6       0.75      0.60      0.67      1218\n",
            "           7       0.94      0.94      0.94      1159\n",
            "           8       0.96      0.97      0.96      1197\n",
            "           9       0.95      0.96      0.95      1203\n",
            "\n",
            "    accuracy                           0.88     12000\n",
            "   macro avg       0.88      0.88      0.88     12000\n",
            "weighted avg       0.88      0.88      0.88     12000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83      1000\n",
            "           1       0.99      0.96      0.97      1000\n",
            "           2       0.77      0.80      0.78      1000\n",
            "           3       0.87      0.90      0.89      1000\n",
            "           4       0.76      0.81      0.79      1000\n",
            "           5       0.98      0.96      0.97      1000\n",
            "           6       0.71      0.59      0.64      1000\n",
            "           7       0.93      0.95      0.94      1000\n",
            "           8       0.96      0.97      0.96      1000\n",
            "           9       0.95      0.95      0.95      1000\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred_val = clf.predict(X_val)\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "y_pred_test = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kORYhaXqPGS3"
      },
      "source": [
        "##4. k-Nearest Neighbors (kNN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I34_mQYwAtHl",
        "outputId": "4f1ede22-98f8-4169-ae9a-9d86151a5876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.86      0.80      1202\n",
            "           1       0.99      0.97      0.98      1219\n",
            "           2       0.74      0.80      0.77      1205\n",
            "           3       0.88      0.86      0.87      1184\n",
            "           4       0.78      0.75      0.77      1202\n",
            "           5       0.99      0.85      0.91      1211\n",
            "           6       0.66      0.59      0.62      1218\n",
            "           7       0.89      0.96      0.92      1159\n",
            "           8       0.98      0.93      0.95      1197\n",
            "           9       0.90      0.97      0.93      1203\n",
            "\n",
            "    accuracy                           0.85     12000\n",
            "   macro avg       0.86      0.85      0.85     12000\n",
            "weighted avg       0.85      0.85      0.85     12000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.84      0.79      1000\n",
            "           1       0.98      0.97      0.97      1000\n",
            "           2       0.73      0.79      0.76      1000\n",
            "           3       0.90      0.85      0.87      1000\n",
            "           4       0.77      0.74      0.76      1000\n",
            "           5       0.98      0.85      0.91      1000\n",
            "           6       0.62      0.58      0.60      1000\n",
            "           7       0.89      0.95      0.92      1000\n",
            "           8       0.98      0.93      0.95      1000\n",
            "           9       0.90      0.96      0.93      1000\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred_val = clf.predict(X_val)\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "y_pred_test = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcLGDa1qPKHu"
      },
      "source": [
        "# 5. Gradient Boosting Machines (GBM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjNHwevRMeTl"
      },
      "source": [
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUgLaB34MX5D"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUtRiBZ8AtSR",
        "outputId": "ae7f554b-4d2e-4457-f6ef-f08dcdaf74be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86      1202\n",
            "           1       1.00      0.98      0.99      1219\n",
            "           2       0.83      0.85      0.84      1205\n",
            "           3       0.89      0.92      0.91      1184\n",
            "           4       0.83      0.84      0.84      1202\n",
            "           5       0.98      0.97      0.98      1211\n",
            "           6       0.77      0.69      0.73      1218\n",
            "           7       0.96      0.96      0.96      1159\n",
            "           8       0.98      0.97      0.97      1197\n",
            "           9       0.97      0.97      0.97      1203\n",
            "\n",
            "    accuracy                           0.90     12000\n",
            "   macro avg       0.90      0.90      0.90     12000\n",
            "weighted avg       0.90      0.90      0.90     12000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.85      1000\n",
            "           1       0.99      0.97      0.98      1000\n",
            "           2       0.79      0.82      0.81      1000\n",
            "           3       0.90      0.90      0.90      1000\n",
            "           4       0.81      0.83      0.82      1000\n",
            "           5       0.99      0.97      0.98      1000\n",
            "           6       0.72      0.66      0.69      1000\n",
            "           7       0.95      0.97      0.96      1000\n",
            "           8       0.97      0.97      0.97      1000\n",
            "           9       0.96      0.97      0.96      1000\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "clf = XGBClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred_val = clf.predict(X_val)\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "y_pred_test = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YerOvDp0MbXo"
      },
      "source": [
        "#6. Exploring Neural Network more (Convolutional Neural Networks)\n",
        "\n",
        "1.   Add Multiple Layers\n",
        "2.   or we can try ResNet(if it's needed , probably NOT!!)\n",
        "3.   check multiple opimizer also check gradient learning rates.\n",
        "4.   add more data augmention.\n",
        "5.   maxpool avgpool\n",
        "\n",
        "add more visualizition for the report\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTS4rzuYvXcJ",
        "outputId": "5277f3f6-c3ac-4560-fcbd-c17ebdcb8d93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.303\n",
            "[1,  4000] loss: 2.302\n",
            "[1,  6000] loss: 2.302\n",
            "[1,  8000] loss: 2.301\n",
            "[1, 10000] loss: 2.300\n",
            "[1, 12000] loss: 2.299\n",
            "[2,  2000] loss: 2.298\n",
            "[2,  4000] loss: 2.295\n",
            "[2,  6000] loss: 2.290\n",
            "[2,  8000] loss: 2.280\n",
            "[2, 10000] loss: 2.257\n",
            "[2, 12000] loss: 2.205\n",
            "[3,  2000] loss: 2.126\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define complex network for FashionMNIST\n",
        "class ComplexFashionMNISTNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ComplexFashionMNISTNet, self).__init__()\n",
        "        # Define layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc1 = nn.Linear(128, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional layers with ReLU and pooling\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.avgpool(x)  # Global Average Pooling\n",
        "        x = torch.flatten(x, 1)\n",
        "        # Fully connected layers with dropout\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "    transforms.RandomErasing()\n",
        "])\n",
        "\n",
        "\n",
        "model = ComplexFashionMNISTNet().to(device)\n",
        "\n",
        "\n",
        "\n",
        "#enable GPU\n",
        "model = ComplexFashionMNISTNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "\n",
        "#train\n",
        "LOSSES = []\n",
        "EPOCHES = []\n",
        "for epoch in range(100):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "    LOSSES.append(running_loss)\n",
        "    EPOCHES.append(epoch)\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA6rvDA17vAx"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "y_pred= []\n",
        "y_true=[]\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=testset.classes, digits=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBqYHAxeHhmm"
      },
      "outputs": [],
      "source": [
        "# visualize activation in\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "model.conv3.register_forward_hook(get_activation('conv2'))\n",
        "model.fc3.register_forward_hook(get_activation('fc3'))\n",
        "\n",
        "images, _ = next(iter(testloader))\n",
        "output = model(images)\n",
        "\n",
        "def visualize_feature_maps(layer_activations):\n",
        "    num_feature_maps = layer_activations.size(1)\n",
        "    fig, axarr = plt.subplots(min(num_feature_maps, 4))\n",
        "    for idx in range(min(num_feature_maps, 4)):\n",
        "        feature_map = layer_activations[0][idx]\n",
        "        axarr[idx].imshow(feature_map.cpu().numpy(), cmap=\"gray\")\n",
        "        axarr[idx].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "visualize_feature_maps(activation['conv2'])\n",
        "\n",
        "def visualize_fc_layer(layer_weights):\n",
        "    plt.imshow(layer_weights.cpu().numpy(), cmap=\"coolwarm\")\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "visualize_fc_layer(model.fc3.weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CID7ltmHhu3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip0mX4diHigc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}